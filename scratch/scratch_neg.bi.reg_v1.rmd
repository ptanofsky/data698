---
title: "Scratch Negative Binomial Regression"
author: "Philip Tanofsky"
date: "`r Sys.Date()`"
output: html_document
---

# Starting with tutorial here:

https://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/

```{r}
library(foreign)
library(ggplot2)
library(MASS)
```

```{r}
dat <- read.dta("https://stats.idre.ucla.edu/stat/stata/dae/nb_data.dta")
```

```{r}
dat <- within(dat, {
  prog <- factor(prog, levels=1:3, labels=c("General", "Academic", "Vocational"))
  id <- factor(id)
})

summary(dat)
```

```{r}
ggplot(dat, aes(daysabs, fill=prog)) +
  geom_histogram(binwidth = 1) +
  facet_grid(prog ~ ., margins = TRUE, scales = "free")
```

```{r}
with(dat, tapply(daysabs, prog, function(x) {
  sprintf("M (SD) = %1.2f (%1.2f)", mean(x), sd(x))
}))
```

Negative binomial regression - Negative binomial regression can be used for over-dispersed count data, that is when the conditional variance exceeds the conditional mean. It can be considered as a generalization of Poisson regression since it has the same mean structure as Poisson regression and it has an extra parameter to model the over-dispersion. If the conditional distribution of the outcome variable is over-dispersed, the confidence intervals for the Negative binomial regression are likely to be wider as compared to those from a Poisson regression model.

Poisson regression - Poisson regression is often used for modeling count data. Poisson regression has a number of extensions useful for count models.

Zero-inflated regression model - Zero-inflated models attempt to account for excess zeros. In other words, two kinds of zeroes are thought to exist in the data, "true zeros" and "excess zeros". Zero-inflated models estimate two equations simultaneously, one of the count model and one for the excess zeros.

OLS regression - Count outcome variables are sometimes log-transformed and analyzed using OLS regression. many issues arise with this approach, including loss of data due to undefined values generated by taking the log of zero (which is undefined), as well as the lack of capacity to model the dispersion.


```{r}
summary(m1 <- glm.nb(daysabs ~ math + prog, data = dat))
```

```{r}
m2 <- update(m1, . ~ . - prog)
anova(m1, m2)
```

## Checking model assumption

```{r}
m3 <- glm(daysabs ~ math + prog, family = "poisson", data=dat)
pchisq(2 * (logLik(m1) - logLik(m3)), df=1, lower.tail=FALSE)
```

```{r}
(est <- cbind(Estimate = coef(m1), confint(m1)))
```

```{r}
exp(est)
```

```{r}
newdata1 <- data.frame(math = mean(dat$math), prog=factor(1:3, levels=1:3,
          labels=levels(dat$prog)))
newdata1$phat <- predict(m1, newdata1, type="response")
newdata1
```

```{r}
newdata2 <- data.frame(
  math = rep(seq(from=min(dat$math), to=max(dat$math), length.out=100), 3),
  prog = factor(rep(1:3, each=100), levels=1:3, labels=levels(dat$prog)))

newdata2 <- cbind(newdata2, predict(m1, newdata2, type="link", se.fit=TRUE))
newdata2 <- within(newdata2, {
  DaysAbsent <- exp(fit)
  LL <- exp(fit - 1.96 * se.fit)
  UL <- exp(fit + 1.96 * se.fit)
})

ggplot(newdata2, aes(math, DaysAbsent)) +
  geom_ribbon(aes(ymin=LL, ymax=UL, fill=prog), alpha=0.25) + 
  geom_line(aes(color=prog), size=2) +
  labs(x="Math Score", y="Predicted Days Absent")
```

## ZERO-INFLATED NEGATIVE BINOMIAL REGRESSION

https://stats.oarc.ucla.edu/r/dae/zinb/

```{r}
library(ggplot2)
library(pscl)
library(MASS)
library(boot)
```

```{r}
zinb <- read.csv("https://stats.idre.ucla.edu/stat/data/fish.csv")
zinb <- within(zinb, {
  nofish <- factor(nofish)
  livebait <- factor(livebait)
  camper <- factor(camper)
})

summary(zinb)
```

```{r}
ggplot(zinb, aes(count, fill=camper)) +
  geom_histogram() +
  scale_x_log10() +
  facet_grid(camper ~ ., margins=TRUE, scales="free_y")
```

```{r}
m1 <- zeroinfl(count ~ child + camper | persons,
  data = zinb, dist = "negbin")
summary(m1)
```

```{r}
m0 <- update(m1, . ~ 1)
pchisq(2 * (logLik(m1) - logLik(m0)), df=3, lower.tail=FALSE)
```

```{r}
dput(round(coef(m1, "count"), 4))
```

```{r}
dput(round(coef(m1, "zero"), 4))
```

```{r}
f <- function(data, i) {
  require(pscl)
  m <- zeroinfl(count ~ child + camper | persons,
                data=data[i, ], dist="negbin",
                start=list(count = c(1.3711, -1.5152, 0.879), zero = c(1.6028, -1.6663)))
  as.vector(t(do.call(rbind, coef(summary(m)))[, 1:2]))
}

set.seed(10)
(res <- boot(zinb, f, R=1200, parallel="snow", ncpus=4))
```

```{r}
# Basic parameter estimates with percentile and bias adjusted CIs
parms <- t(sapply(c(1,3,5,9,11), function(i) {
  out <- boot.ci(res, index=c(i, i+1), type=c("perc", "bca"))
  with(out, c(Est=t0, pLL=percent[4], pUL=percent[5],
              bcaLL = bca[4], bcaUL = bca[5]))
}))

# add row names
row.names(parms) <- names(coef(m1))
# output results
parms
```

```{r}
# compare with normal based approximation
confint(m1)
```

```{r}
## exponentiated parameter estimates with percentile and bias adjusted CIs
expparms <- t(sapply(c(1,3,5,7,9), function(i) {
  out <- boot.ci(res, index=c(i, i+1), type=c("perc", "bca"), h=exp)
  with(out, c(Est=t0, pLL=percent[4], pUL=percent[5],
              bcaLL=bca[4], bcaUL=bca[5]))
}))

# add row names
row.names(expparms) <- names(coef(m1))

# Output results
expparms
```

```{r}
newdata1 <- expand.grid(0:3, factor(0:1), 1:4)
colnames(newdata1) <- c("child", "camper", "persons")
newdata1$phat <- predict(m1, newdata1)

ggplot(newdata1, aes(x=child, y=phat, color=factor(persons))) +
  geom_point() +
  geom_line() +
  facet_wrap(~camper) +
  labs(x="Number of Children", y="Predicted Fish Caught")
```

# HIERARCHICAL CLUSTERING

https://www.r-bloggers.com/2019/06/hierarchical-clustering-for-location-based-strategy-using-r-for-e-commerce/


```{r}
# Note: I don't have access to the dataset but typing anyways to familiarize myself with the functions used

# mean of Lat Lon
mean.lat <- mean(HotelsCity$latitude, na.rm=TRUE)
mean.lon <- mean(HotelsCity$longitude, na.rm=TRUE)

# Distance of all hotels from mean lat lon
hotels.lat.lon <- HotelsCity[,c(4.5)]
mean.lat.lon <- data.frame(mean.lat, mean.lon)
distance_mat <- distm(hotels.lat.lon[2:1], mean.lat.lon[2:1], FUN=distHaversine)
distance_mat <- as.data.frame(distance_mat)
```

```{r}
# Calculating Cutoff Distance for Outlier Removal
iqr <- IQR(as.numeric(distance_mat[,1]), na.rm=TRUE)

cutoff <- as.numeric(quantile(distance_mat$V1, 0.75, na.rm=TRUE) + iqr*1.5)

HotelDetail$Flag <- ifelse(HotelDetails$V1>Cutoff, "Incorrect", "Correct")
Outliers_Final <- filter(HotelDetail, Flag=="Incorrect")
```

```{r}
# Now let's create a distance matrix
# Distance Matrix for city
distance_mat <- distm(hotels.lat.lon[2:1], hotels.lat.lon[2:1], FUN=distHaversine)
distance_mat <- as.data.frame(distance_mat)
distance_mat[is.na(distance_mat)] <- 0
dMat <- as.dist(distance_mat)
```

```{r}
# Hierarchicical clustering
hc <- hclust(dMat, method="complete")
HotelCity_Valid$Clusters <- cutree(hc, h=AvgDist2)
```

From: https://gis.stackexchange.com/questions/64392/finding-clusters-of-points-based-distance-rule-using-r

You can use a hierarchical clustering approach. By applying hclust and cutree you can derive clusters that are within a specified distance.

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```


~~~~~

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```
