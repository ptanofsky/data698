---
title: "Scratch with Sept 2022 Data"
author: "Philip Tanofsky"
date: "`r Sys.Date()`"
output: html_document
---

Scratch work using the September 2022 data, most recent as I'm working on this Oct. 30, 2022

Update: v2, started working at 8:30p on Friday, Nov. 11, 2022


Note to self: JC prefixed files are Jersey City, and thus it's all New Jersey info, so omit that, just focus on NYC data (no prefix)


11/11/2022

TODO

- Geographical clustering

- Clustering by Matrix (based on edges)
  - Network graph to find the hubs and spanners
  
- Historical data: figure out rebalancing
  - Use the blog post from the Python guy to understand how to discern and how to apply
  
- Data from chron script
  - By Sunday morning I will have almost two weeks of full data
  - When is rebalancing occuring based on availability count each 15 minutes
  
** How to combine the 3 datasets into one understanding?
  - Current Ride trips
  - Historical ride trips
  - Availability data by station
  ** Determine final dataset for model, current but how far back?
  
- Subway stops as related to Citibike activity

- MODELS
  - Find 4-5 models from the Lit Review and try them on the dataset
  - Negative binomial clustering
  - Zero-inflated 
  - 3
  - 4
  
** But what will be the most important factor? If it's linear regression that won't work

** Do I need to include another piece of data?

- Consider boroughs

- Does R have a NYC neighborhood lookup?

- How does elevation impact bike trips?

- If I focus by station what can I learn if station focused?




```{r}
# Required packages
library(tidyverse)
library(ggplot2)
library(skimr)
#library(tidyjson)
#library(geosphere)
library(lubridate)
# library(ggmap)
#library(forcats)
#library(scales)
```

```{r}
# Let's create some other columns of value
# weekday
# day of the month
# trip duration in minutes
# start hour
citibike <- read.csv("data/202209-citibike-tripdata.csv", check.names=TRUE) %>%
  mutate(day = factor(mday(ymd_hms(started_at))),
         start.hour=factor(hour(ymd_hms(started_at))),
         weekday = wday(ymd_hms(started_at),label=TRUE,abbr=TRUE),
         trip.duration = as.numeric(difftime(ended_at,started_at,units="mins")),
         member_casual = factor(member_casual))

#citibike
```

Columns
- ride_id: Unique identifier of the ride, probably not meaningful to me
- rideable_type:

```{r}
nrow(citibike)
```

September contains 3,507,123 rows, so 3.5 million trips

```{r}
skim(citibike)
```

```{r}
citibike %>%
  filter(is.na(end_lat))
```

It appears 3838 bike trips were abandoned as the end_lat, end_long, end_station_name, and end_station_id are empty.

```{r}
# Number of trips for each hour of the day
ggplot(citibike, aes(x=start.hour)) +
  geom_bar() +
  labs(x = 'Time of Day',
       y = 'Number of Trips') +
  theme(axis.text.x = element_text(size=8, angle=90))

```

This is overall user base, so a spike at 8a and then again at 5p and 6p hour, which is expected for rush hour, to and from work.

```{r}
# Now let's check trips per hour by user type
ggplot(citibike, aes(x=start.hour)) +
  geom_bar() +
  labs(x = 'Time of Day',
       y = 'Number of Trips') +
  theme(axis.text.x=element_text(size=8, angle=90)) +
  facet_wrap(~member_casual)
```

Rush hour spikes for members, also a bit of a evening rush hour for casual, too. The separation of member vs casual shows a clearer spike at the rush hour times

```{r}
# Trips by weekday, total count
ggplot(citibike, aes(x=weekday)) +
  geom_bar() +
  labs(x = 'Day of Week',
       y = 'Number of Trips',
       title = 'Number of Bike Trips by Day of Week')
```

Greater on Thu and Friday, but is that due to extra days of the month ... I bet it is.

```{r}
citibike %>%
  group_by(day) %>%
  summarize(n=n(),
            weekday = weekday[1]) %>%
  group_by(weekday) %>%
  summarize(n.m=mean(n)) %>% # Get the average for each day to account for extra days of the month beyond 28
  ggplot(aes(x=weekday, y=n.m)) + geom_bar(stat='identity') +
  labs(x = 'Day of Week',
       y = 'Number of Trips',
       title = 'Average Number of Bike Trips by Day of Week')
```

Yep, that lowers the Thu and Fri count. And now it appears Wed is the highest volume day

```{r}
# Let's check the volume of day by the user type
citibike %>%
  group_by(day, member_casual) %>%
  summarize(n=n(),
            weekday = weekday[1]) %>%
  group_by(weekday, member_casual) %>%
  summarize(n.m=mean(n)) %>%
  ggplot(aes(x=weekday, y=n.m)) + geom_bar(stat='identity') +
  labs(x = 'Day of Week',
       y = 'Number of Trips',
       title = 'Number of Bike Trips by Day of Week') +
  facet_wrap(~member_casual)
```

Casual is more likely tourists and recreation use, while member are more likely to use during weekdays, and thus more likely for work

```{r}
# Trip by weekday by segment by time of day
citibike %>%
  group_by(day, member_casual, start.hour) %>%
  summarize(n=n(),
            weekday=weekday[1]) %>%
  group_by(weekday, member_casual, start.hour) %>%
  summarize(n.m=mean(n)) %>%
  ggplot(aes(x=start.hour, y=n.m, fill=weekday)) +
  geom_bar(stat='identity') +
  labs(x='Time of Day',
       y='Number of Trips',
       title='Average Number of Bike Trips by Time of Day and Weekday') +
  facet_grid(weekday~member_casual) +
  theme(axis.text.x = element_text(size=8, angle=90),
        legend.position = 'none')
```

Here, we see member accounts use the bikes on the weekend greater than casual. Interesting, for casual we do see spikes during the end of workday rush hour on Tue, Wed, Thu, Fri.

```{r}
# Trip durations
# Consider bike trips less than 100 minutes as the checkout time is 30 minutes, so this is double the allotted time.
citibike %>%
  filter(trip.duration < 100) %>% # only consider trips less than 100 minutes. 
  ggplot(aes(x=trip.duration)) +
  geom_histogram(bins=100) +
  labs(x = 'Trip Duration (min.)',
       y = 'Number of Trips')

```

As expected, most trips are below 30 minutes and the graph is left skewed, indicating a higher volume of short trip durations.

```{r}
# Segment durations by user type
citibike %>%
  filter(trip.duration < 100) %>%
  ggplot(aes(x=trip.duration)) +
  geom_histogram(bins=100) +
  labs(x = 'Trip Duration (min.)',
       y = 'Number of Trips') +
  facet_wrap(~member_casual)
```

```{r}
# Same as above with density plot instead of histogram
citibike %>%
  filter(trip.duration < 100) %>%
  ggplot(aes(x=trip.duration, fill=member_casual)) +
  geom_density(alpha=0.2) +
  labs(x = 'Trip Duration (min.)',
       y = 'Number of Trips')
  
```

Casual do tend to have longer trips

```{r}
# Ok, now I want to understand all the stations
n_distinct(citibike$start_station_id)

citibike %>%
  filter(startsWith(as.character(start_station_id), 'S'))
# Hmmm, there are 58 rows in the NYC district that start with 'SYS'
```

1618 start station IDs in NYC

```{r}
n_distinct(citibike$end_station_id)
```

And 1658 end station IDs in NYC

```{r}
setdiff(citibike$end_station_id, citibike$start_station_id)
```

Of those 40, there is a blank, which means abandoned, several JC (Jersey City), several HB (Hoboken), one number which should be a specific station in NYC, and 3 SYS*

```{r}
# Create table of start to end station IDs
stations_cols <- citibike %>%
  filter(trip.duration > 4) %>% # Only consider trips of a minimum 5 minutes
  select(start_station_id, end_station_id)
stations_table <- as.data.frame((table(stations_cols)))

stations_table <- stations_table %>%
  filter(Freq > 0)

stations_table_order <- stations_table[order(-stations_table$Freq),]

stations_table_order
```

For all valid combinations of start to end, there are 472920 combinations
By removing trips of <= 4, there's a total of 471130 combinations

The above table also indicates the frequency of those trips for start-end combinations.

The top 5 entries are the same start and end station ID. so these are casual rides, I have a feeling these 5 are near tourist destinations, perhaps Central Park

```{r}
stations_table_dif <- stations_table_order %>%
  filter(as.character(start_station_id) != as.character(end_station_id))

nrow(stations_table_dif)
```

469531 in which the start and the end station IDs are NOT the same

```{r}
tail(stations_table_dif, 200)
```

Now, I want to determine overall flow between any existing combination. At the end of the month, is the flow to or from a station

```{r}
stations_table_dif1 <- stations_table_dif
stations_table_dif2 <- stations_table_dif
```



```{r}
# Added all=TRUE to account for one-sided counts
res <- 
merge(stations_table_dif1, 
      stations_table_dif1, 
      by.x=c('start_station_id','end_station_id'), 
      by.y=c('end_station_id','start_station_id'), all = TRUE)
# Set the NA (one-sided trips) to count of 0
res[is.na(res)] <- 0
```

```{r}
res$surplus <- res$Freq.y - res$Freq.x
res <- res[order(-res$surplus),]
res
```

```{r}
# Remove rows with no start station id, those are a result of the merge all=TRUE and should not exist
# Also, remove all rows in which the start_station_id begins with an 'S'
res <- res %>% filter(start_station_id != '' & !startsWith(as.character(start_station_id), 'S'))
res
```

Above returns a list of 618,835 combinations given every start-end combo exists as the inverse, 

So Freq.x is the count of trips from start to end
-- Freq.y is the count of trips from end to start
surplus is Freq.y - Freq.x which means that positive number indicates return trips are more and thus surplus of bikes
and negative number indicates return trips are less and thus the station is not breaking even

```{r}
res %>% filter(end_station_id == '')
```

1350 rows indicate most start stations will have at least one abandoned bike

```{r}
res %>% filter(Freq.x == 0)
```

With Freq.x ==0 , there are 149336 rows in which an end-to-start occurs, so the rows are pure surplus to the start_station_id

```{r}
# Want to identify the surplus (or not) by station for the month
station_surplus_count <- res %>%
  group_by(start_station_id) %>%
  summarize(surplus_sum=sum(surplus))
station_surplus_count
```
The above indicates 1650 rows, 8 less than the count of end_station_id from the initial dataset

```{r}
station_surplus_count <- station_surplus_count[order(-station_surplus_count$surplus_sum),]
#station_surplus_count %>%
#    filter(startsWith(as.character(start_station_id), 'S'))
colnames(station_surplus_count)[1] <- "station_id"
station_surplus_count
```
21 start with J
14 start with H

Above is result showing the surplus(+) or not surplus (-) of 1650 stations in the NYC district, this probably includes a few JC or HB stations

```{r}
# Extract just the end station Id, lat, log
# because this had the higher count from the initial dataset, going with end_station_id
end_station_info <- citibike %>%
  select(end_station_id, end_lng, end_lat)

#end_station_info <- unique( end_station_info[ , 1:3 ] )
end_station_info <- end_station_info[!duplicated(end_station_info$end_station_id),]
end_station_info
```

```{r}
# Now I want the coordinates of all those station_ids
station_surplus_count_coords <- merge(x = station_surplus_count, y = end_station_info, by.x = 'station_id', by.y='end_station_id', all.x = TRUE)

colnames(station_surplus_count_coords)[3] <- "lng"
colnames(station_surplus_count_coords)[4] <- "lat"
station_surplus_count_coords
```
Above table accounts for all the station IDs with overal surplus or not surplus along with coordinates

now to try leaflet to color the station Ids

```{r}
library(leaflet)

```

```{r}

# Using basemaps for NYC
m <- leaflet(data=station_surplus_count_coords) %>% 
#  setView(zoom=12) %>%
  addTiles() %>%
  addCircleMarkers(
    ~lng, ~lat, 
    popup=~as.character(station_id), 
    label=~as.character(station_id), 
    radius=.5,
    color = ~ifelse(surplus_sum >= 1, 'blue', 
                    ifelse(surplus_sum == 0, 'green', 'red'))
  )

# Display map
m
```

Above map indicates the overall net bikes of a station for the given month
- blue, value is positive
- red, value is negative
- green, value is 0

```{r}
sum(station_surplus_count_coords$surplus_sum)
```


From the initial dataset, there were 3838 trips without an end lat/long, so that means bikes were abandoned
And doing the math 3838 * 2 the 7676, which is just 10 more than the negative number, so it seems I've included the abandoned in this surplus.

If the surplus is negative, this must account for the abandoned bikes, but why is the count double? I should set blank to ABAN so it's clear there's a value to it.

```{r}
station_surplus_count_coords %>%
  filter(surplus_sum > 0) %>%
  count()
```
826 stations with a surplus

```{r}
station_surplus_count_coords %>%
  filter(surplus_sum == 0) %>%
  count()
```
21 stations that break even

```{r}
station_surplus_count_coords %>%
  filter(surplus_sum < 0) %>%
  count()
```
803 stations with not surplus (overall result is less bikes)

```{r}
head(station_surplus_count)
```

```{r}

```

```{r}
# Now I want the coordinates of all those station_ids
station_surplus_count_coords <- merge(x = station_surplus_count, y = citibike, by.x = 'station_id', by.y='end_station_id', all.x = TRUE)
station_surplus_count_coords
```

```{r}

```

```{r}
# Count of occurrences for each value in start_station_id
start_station_id_counts <- as.data.frame(table(citibike$start_station_id))

colnames(start_station_id_counts)[1] <- "start_station_id"
colnames(start_station_id_counts)[2] <- "start_count"
start_station_id_counts <- start_station_id_counts[order(-start_station_id_counts$start_count),]
start_station_id_counts
```

```{r}
# Count of occurrences for each value in end_station_id
as.data.frame(table(citibike$end_station_id))

end_station_id_counts <- as.data.frame(table(citibike$end_station_id))

colnames(end_station_id_counts)[1] <- "end_station_id"
colnames(end_station_id_counts)[2] <- "end_count"
end_station_id_counts <- end_station_id_counts[order(-end_station_id_counts$end_count),]
end_station_id_counts
```

```{r}
res <- merge(start_station_id_counts, end_station_id_counts, by.x=c('start_station_id'), by.y=c('end_station_id'))
colnames(res)[1] <- "station_id"
res$end_minus_start_cnt <- res$end_count - res$start_count
res <- res[order(-res$end_minus_start_cnt),]
res
```

Positive number indicates surplus by bikes
Negative number indicates bikes not being returned to a station

```{r}
min(citibike$started_at)
```

```{r}
# https://stackoverflow.com/questions/36827572/create-a-time-interval-of-15-minutes-from-minutely-data-in-r
citibike.start.count.by.station <- citibike %>% group_by(by15=cut(as.POSIXct(started_at), "15 min")) %>%
  dplyr::count(start_station_id, by15, sort=TRUE)

citibike.start.count.by.station
```

```{r}
citibike.end.count.by.station <- citibike %>% group_by(by15=cut(as.POSIXct(ended_at), mydateseq)) %>%
  dplyr::count(end_station_id, by15, sort=TRUE)

citibike.end.count.by.station
```

```{r}
station.count.by.15min <- 
  merge(citibike.start.count.by.station, citibike.end.count.by.station, by.x=c('by15', 'start_station_id'), by.y=c('by15', 'end_station_id'))

station.count.by.15min
```

```{r}
mydateseq<-seq(as.POSIXct("2022-09-01"), by="15 min", length.out = 2880)  

# Output the data sequence to confirm the result are 15 minute intervals which start on the hour
#mydateseq
```

```{r}
station.count.by.15min$diff <- station.count.by.15min$n.y - station.count.by.15min$n.x

station.count.by.15min
```

```{r}

```

```{r}

```

```{r}

```

# NEXT: Plot the stations
Once I have the above info, then I can see which stations have greater volume based on location

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

~~~~~

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```
