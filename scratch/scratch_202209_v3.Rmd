---
title: "Scratch with Sept 2022 Data"
author: "Philip Tanofsky"
date: "`r Sys.Date()`"
output: html_document
---

Scratch work using the September 2022 data, most recent as I'm working on this Oct. 30, 2022

Update: v2, started working at 8:30p on Friday, Nov. 11, 2022

Updated: v3, started working at 1:44P on Saturday, Nov. 12, 2022


11/12/2022

Data Collection and Analysis

For submission on 11/13/2022
      + scratch_station_avail_v1.rmd does lookup of station lat/lon to neighborhood and borough
  ++ Add some charts for Sept. '22 avail by borough and neighborhood
      + scratch_elevation_distance_v1.rmd determines elevation of station, add that to above
  ++ Add some charts for Sept. '22 avail by elevation
      + scratch_network_graph_map_v1.rmd builds network graph based on trips start/ends
  ++ Perform some initial network graph analysis (nothing important expected here)
      + time_series_graphs_v1.rmd
        + Add these but make sure they aren't duplicate of some initial charts
      + leaflet for shiny
  ++ small multiples of surplus by hour, perhaps 7a-7p (12-15 total)
+ matrix_15min_intervals_v1 likely needed for above

++ Do some sort of clustering!!!



Not yet, probably Monday
Given the two weeks of API data, determine when the rebalance will occur for any station and then synthetically add that to the running totals by adding in a fake row with a time + 1 minute and the surplus to help reset the running count


Input into the model
Lat/long, day of the week, time, bikes desired, within XX meters (distance willing to walk)



```{r warning=FALSE}
# Required packages
library(tidyverse)
library(ggplot2)
library(skimr)
library(lubridate)
library(fpp3)
library(assertthat)
library(igraph)
library(ggraph)
library(ggmap)
library(leaflet)
library(rgdal)
library(RColorBrewer)
```


Analysis starting with the Citibike bike trips for September 2022.

```{r}
# Create additional columns of pertinence
# weekday, day of the month, trip duration in minutes, start hour
citibike <- read.csv("data/202209-citibike-tripdata.csv", check.names=TRUE) %>%
  mutate(day = factor(mday(ymd_hms(started_at))),
         start.hour=factor(hour(ymd_hms(started_at))),
         weekday = wday(ymd_hms(started_at),label=TRUE,abbr=TRUE),
         trip.duration = as.numeric(difftime(ended_at,started_at,units="mins")),
         member_casual = factor(member_casual))
#%>%
#  rename(ride.id=ride_id, rideable.type=rideable_type, started.at=started_at,
#         ended.at=ended_at, start.station.name=start_station_name, start.station.id=start_station_id,
#         end.station.name=end_station_name, end.station.id=end_station_id, start.lat=start_lat,
#         start.lng=start_lng, end.lat=end_lat, end.lng=end_lng, member.casual=member_casual)

# Output DF if needed
#head(citibike)
```

Columns
- ride_id: Unique identifier of the ride

- rideable_type: class vs electric bike vs docked bike (still not sure the purpose of docked bike)

- started_at: Timestamp of trip departure

- ended_at: Timestamp of trip arrival

- start_station_name: Name of station of trip departure

- start_station_id: Unique identifier of station of trip departure

- end_station_name: Name of station of trip arrival

- end_station_id: Unique identifier of station of trip arrival

- start_lat: Latitude of departure location

- start_lng: Longitude of departure location

- end_lat: Latitude of arrival location

- end_lng: Longitude of arrival location

- member_casual: Member or casual user type

- day: Day of the month

- start.hour: Hour of the trip departure

- weekday: Day of the week for the trip

- trip.duration: Duration of bike trip in minutes.

```{r}
# Output DF to visually check data
# citibike
```

September contains 3,507,123 rows, so 3.5 million trips during the month of September 2022.

```{r}
skim(citibike)
```

3507123 ride_id's which is one for each bike trip

end_station_name and end_station_id are empty for 8012 rows
end_lat and end_lng are missing for 3838 rows

With 8012 missing end_station_name and end_station_id, it would indicate 8012 trips can be considered abandoned in that they were not formally docked. 4174 contain the end_lat and end_lng so final location is available but for 48% of abandons, the final destination is not included in the data. Consider all 8012 as 'Abandons'

From: https://ride.citibikenyc.com/system-data

This data has been processed to remove trips that are taken by staff as they service and inspect the system, trips that are taken to/from any of our “test” stations (which we were using more in June and July 2013), and any trips that were below 60 seconds in length (potentially false starts or users trying to re-dock a bike to ensure it's secure).

```{r}
citibike %>%
  group_by(rideable_type) %>%
  summarize(n=n())
```

Docked_bike is 28432 which is almost 1000 per day, does this represent the rebalancing. No, the rebalancing for August 2022 was 78,782 bicycles during the month of August at (https://mot-marketing-whitelabel-prod.s3.amazonaws.com/nyc/August-2022-Citi-Bike-Monthly-Report.pdf), and given Sept. 2022 is similar in overall bike trip count, no assumption can be made of docked_bike at this time.

```{r}
citibike %>%
  filter(rideable_type=='docked_bike') %>%
  group_by(member_casual) %>%
  summarize(n=n())
```
All 28432 are casual for the 'docked_bike'. This implies they are not member rides.

```{r}
citibike %>%
  filter(end_station_name == '')
```

It appears 8012 bike trips were abandoned as the end_station_name, and end_station_id are empty.

```{r}
citibike %>%
  filter(end_station_name == '') %>%
  group_by(member_casual) %>%
  summarize(count=n())
```

Interesting there are more member abandons then casual. But definition of member could be 1 month, not a whole year.

```{r}
# Number of trips for each hour of the day
ggplot(citibike, aes(x=start.hour)) +
  geom_bar() +
  labs(x = 'Time of Day',
       y = 'Number of Trips') +
  theme(axis.text.x = element_text(size=8, angle=90))
```

This is overall user base, so a spike at 8A and then again at 5P and 6P hour, which is expected for rush hour, to and from work.

```{r}
# Now let's check trips per hour by user type
ggplot(citibike, aes(x=start.hour)) +
  geom_bar() +
  labs(x = 'Time of Day',
       y = 'Number of Trips') +
  theme(axis.text.x=element_text(size=8, angle=90)) +
  facet_wrap(~member_casual)
```

Rush hour spikes for members, also a bit of a evening rush hour for casual, too. The separation of member vs casual shows a clearer spike at the rush hour times. Also, the higher usage of member throughout the middle of the day may indicate even if someone is a member the primary reason may not be transportation to and from work.


```{r}
citibike %>%
  group_by(day) %>%
  summarize(n=n(),
            weekday = weekday[1]) %>%
  group_by(weekday) %>%
  summarize(n.m=mean(n)) %>% # Get the average for each day to account for extra days of the month beyond 28
  ggplot(aes(x=weekday, y=n.m)) + geom_bar(stat='identity') +
  labs(x = 'Day of Week',
       y = 'Number of Trips',
       title = 'Average Number of Bike Trips by Day of Week')
```

By average trips per day of the week, Wednesday is the highest volume day. Given this is September 2022, I wonder if this is a result of pandemic return-to-office policies in which it's more common for workers to return to office during the middle of the week instead of Monday or Friday for those with hybrid schedules.

```{r}
# Let's check the volume of day by the user type
citibike %>%
  group_by(day, member_casual) %>%
  summarize(n=n(),
            weekday = weekday[1]) %>%
  group_by(weekday, member_casual) %>%
  summarize(n.m=mean(n)) %>%
  ggplot(aes(x=weekday, y=n.m)) + geom_bar(stat='identity') +
  labs(x = 'Day of Week',
       y = 'Number of Trips',
       title = 'Number of Bike Trips by Day of Week') +
  facet_wrap(~member_casual)
```

Casual is more likely tourists and recreation use represented in the Sunday and Saturday as the days with the highest volume of trips, while member are more likely to use during weekdays, and thus more likely for work transportation.

```{r}
# Trip by weekday by segment by time of day
citibike %>%
  group_by(day, member_casual, start.hour) %>%
  summarize(n=n(),
            weekday=weekday[1]) %>%
  group_by(weekday, member_casual, start.hour) %>%
  summarize(n.m=mean(n)) %>%
  ggplot(aes(x=start.hour, y=n.m, fill=weekday)) +
  geom_bar(stat='identity') +
  labs(x='Time of Day',
       y='Number of Trips',
       title='Average Number of Bike Trips by Time of Day and Weekday') +
  facet_grid(weekday~member_casual) +
  theme(axis.text.x = element_text(size=8, angle=90),
        legend.position = 'none')
```

Here, we see member accounts use the bikes on the weekend greater than casual. Interesting, for casual we do see spikes during the end of workday rush hour on Tue, Wed, Thu, Fri.

```{r}
# Trip durations
citibike %>%
  filter(trip.duration < 100) %>% # only consider trips less than 100 minutes. 
  ggplot(aes(x=trip.duration)) +
  geom_histogram(bins=100) +
  labs(x = 'Trip Duration (min.)',
       y = 'Number of Trips')

```

As expected, most trips are below 30 minutes and the graph is left skewed, indicating a higher volume of short trip durations. The chart only considers bike trips less than 100 minutes as the checkout time is 30 minutes, so this is double the allotted time. With an additional charge after 30 minutes, the majority of trips are 30 minutes or less.

```{r}
# Segment durations by user type
citibike %>%
  filter(trip.duration < 100) %>%
  ggplot(aes(x=trip.duration)) +
  geom_histogram(bins=100) +
  labs(x = 'Trip Duration (min.)',
       y = 'Number of Trips') +
  facet_wrap(~member_casual)
```

```{r}
# Same as above with density plot instead of histogram
citibike %>%
  filter(trip.duration < 100) %>%
  ggplot(aes(x=trip.duration, fill=member_casual)) +
  geom_density(alpha=0.2) +
  labs(x = 'Trip Duration (min.)',
       y = 'Number of Trips')
```

Separating the density plot by user type shows the casual users tend to have longer trips than member users.


```{r}
citibike %>%
  group_by(start.hour) %>%
  summarize(med.duration=median(trip.duration)) %>%
  ggplot(aes(x=start.hour, y=med.duration)) +
  geom_point() +
  geom_line(aes(group=1), linetype='dotted')
```

The above plot shows the average duration of trips based on the time of the day. The longer average of trips (greater than 10 minutes) tend to occur between noon and midnight.

```{r}
citibike %>%
  group_by(weekday, start.hour) %>%
  summarize(med.duration=median(trip.duration)) %>%
  ggplot(aes(x=start.hour, y=med.duration, group=weekday, color=weekday)) +
  geom_point(size=3) +
  geom_line(size=0.5) +
  facet_wrap(~weekday, nrow=1) +
  theme(legend.position='none') +
  scale_x_discrete(breaks=c(0,3,6,9,12,15,18,21))
```

The above plot shows the average trip duration by hour for each day of the week. The weekday pattern occurs Monday through Thrusday with a slight deviation on Friday evening as the weekend starts. The afternoon and evening hours of Saturday and Sunday show longer trips as likely the result of tourism and recreational trips.

```{r}
citibike %>%
  filter(member_casual %in% c('member', 'casual')) %>%
  group_by(weekday, start.hour, member_casual) %>%
  summarize(med.duration=median(trip.duration)) %>%
  ggplot(aes(x=start.hour, y=med.duration, group=weekday, color=weekday)) +
  geom_point(size=3) +
  geom_line(size=0.5) +
  facet_grid(weekday~member_casual) +
  theme(legend.position='none') +
  scale_x_discrete(breaks=c(0,3,6,9,12,15,18,21))
```

The charts of average trip duration by user type shows a clear distinction. member users tend to average bike trips of 10 minutes or less throughout the week, whereas the casual users have a greater variance in average duration based on time of day and day of the week. Reiterating the tourism and recreational use by casual users, the highest average duration occurs in the afternoon on weekend days.

```{r}
citibike %>%
  filter(member_casual %in% c('member', 'casual')) %>%
  group_by(weekday, start.hour, member_casual) %>%
  summarize(med.duration=median(trip.duration)) %>%
  ggplot(aes(x=start.hour, y=med.duration, group=member_casual, 
             color=member_casual, linetype=member_casual, shape=member_casual)) +
  geom_point(size=2) +
  geom_line(size=0.5) +
  facet_wrap(~weekday, nrow=1) +
  labs(x='Time of Day',
       y='Median Trip Duration') +
  scale_x_discrete(breaks=c(0,6,12,18))
```

This is basically the same as the one above it, I like this one better.

# ATTEMPT AT TIME SERIES GRAPHS USING FFP3 LIBRARY

For the fpp3 library, I need to create a formal timestamp column, so create start.ts variable for that purpose.

```{r}
citibike_ts <- citibike %>%
  mutate(start.ts = as_datetime(started_at)) %>%
  as_tsibble(index=start.ts, key = ride_id)
```

```{r}
citibike_ts[1:20,]
```
Confirming the started_at and start_ts are timestamp format

Hmm, I can't plot the data in this instance format, I need the info to follow a specific pattern interval before charting, so let's group by day and then chart that

```{r}
citibike_by_day <- citibike %>%
  mutate(day=as_date(started_at)) %>%
  group_by(day, member_casual) %>%
  summarize(count=n())

#citibike_by_day

citibike_by_day_ts <- citibike_by_day %>%
  as_tsibble(index=day, key=c(member_casual))

#citibike_by_day_ts
```

```{r}
autoplot(citibike_by_day_ts, count) +
  labs(title = "Ride Trips by Day",
       subtitle = "Citi Bike NYC",
       y = "Ride Trips Counts")
```

Time series for total by day between the two user groups.Doesn't quite follow a weekly series, but close. I know it was very bad weather on Sept. 6 and thus impacted both user groups.

```{r}
# Let's attempt hourly
# First convert started_at into a timestamp format
citibike$started_at <- as_datetime(as.character(citibike$started_at))
#citibike
```



```{r}
citibike_by_hour <- citibike %>%
  mutate(hour=lubridate::floor_date(started_at, "1 hour")) %>%
  group_by(hour, member_casual) %>%
  summarize(cnt=n())
#citibike_by_hour

citibike_by_hour_ts <- citibike_by_hour %>%
  as_tsibble(index=hour, key=c(member_casual))
#citibike_by_hour_ts
```

```{r}
autoplot(citibike_by_hour_ts, cnt) +
  labs(title = "Ride Trips by Hour - Sept. 2022",
       subtitle = "Citi Bike NYC",
       y = "Ride Trips Counts")
```

Above is a good look at the time series display of trip counts per hour for the two user types. A clear daily view

```{r}
citibike_by_hour_ts %>%
  gg_season(cnt, period = "week") +
  labs(y = "Ride Trips",
       title = "Seasonal plot: Weekly Trip Counts for Sept. 2022")
```

Rained all day on Tuesday, Sept 6 2022

```{r}
citibike_by_hour_ts %>%
  #filter(member_casual=='member') %>%
  gg_subseries(cnt, period = 'day') +
  labs(y = "Ride Trips",
       title = "Seasonal plot: Weekly Trip Counts for Sept. 2022")
```

Oh, this represents by hour across each day in which each section represents the full month. Still need to figure this out a bit.

```{r}
citibike_by_hour_ts %>%
  #filter(member_casual=='member') %>%
  gg_subseries(cnt, period = 'hour') +
  labs(y = "Ride Trips",
       title = "Seasonal plot: Weekly Trip Counts for Sept. 2022")
```

-30- time series copy/paste

# START HERE WITH ATTEMPT OF THE NETWORK GRAPH OF CITI BIKE STATIONS

```{r}
# https://www.r-bloggers.com/2018/05/three-ways-of-visualizing-a-graph-on-a-map/
```


```{r}
# Figure out abandoned and label them
citibike$end_station_name[citibike$end_station_name == ''] <- "Abandoned"
citibike$end_station_id[citibike$end_station_id  == ''] <- "ABAN"
```

this means there are 8012 abandoned bikes if the end_station_id is not present, but the lat/long does exist for more than half

```{r}
citibike %>%
  filter(citibike$end_station_id  == 'ABAN')
```
3838 have NA end_lat and end_lng, and thus abandoned and no final destination


```{r}
citibike %>%
  filter(startsWith(end_station_id, 'SYS'))
```

There are start_station_id and end_station_id that start with 'SYS' with a valid lat/long, so keep them

There are NO starts with JC or HB, but there are ends with JC and HB

```{r}
# Create table of start to end station IDs
stations_cols <- citibike %>%
  select(start_station_id, end_station_id)
stations_table <- as.data.frame((table(stations_cols)))

stations_table <- stations_table %>%
  filter(Freq > 0)

stations_table_order <- stations_table[order(-stations_table$Freq),]
nrow(stations_table_order)
```

472920 rows 

```{r}
stations_table_dif <- stations_table_order %>%
  filter(as.character(start_station_id) != as.character(end_station_id))

nrow(stations_table_dif)
```

471313 rows in which the start and end are different values 

```{r}
# Create table of start to end station IDs
stations_cols <- citibike %>%
  select(start_station_id, end_station_id)
stations_table <- as.data.frame((table(stations_cols)))

stations_table <- stations_table %>%
  filter(Freq > 0)

stations_table_order <- stations_table[order(-stations_table$Freq),]
nrow(stations_table_order)
```

For all valid combinations of start to end, there are 472920 combinations

The above table also indicates the frequency of those trips for start-end combinations.

The top 15 entries of the top 20 are the same start and end station ID. so these are casual rides, I have a feeling these 5 are near tourist destinations, perhaps Central Park

```{r}
stations_table_dif <- stations_table_order %>%
  filter(as.character(start_station_id) != as.character(end_station_id))

nrow(stations_table_dif)
```

471313 in which the start and the end station IDs are NOT the same

```{r}
tail(stations_table_dif, 200)
```

Now, I want to determine overall flow between any existing combination. At the end of the month, is the flow to or from a station

```{r}
stations_table_dif1 <- stations_table_dif
stations_table_dif2 <- stations_table_dif
```



```{r}
# Added all=TRUE to account for one-sided counts
res <- 
merge(stations_table_dif1, 
      stations_table_dif1, 
      by.x=c('start_station_id','end_station_id'), 
      by.y=c('end_station_id','start_station_id'), all = TRUE)
# Set the NA (one-sided trips) to count of 0
res[is.na(res)] <- 0
```

```{r}
res$surplus <- res$Freq.y - res$Freq.x
res <- res[order(-res$surplus),]
#res
```

```{r}
# Remove rows with start station id equal to ABAN for abandoned, those are a result of the merge all=TRUE and should not exist
res <- res %>% filter(start_station_id != 'ABAN')
#res
```

Above returns a list of 619887 combinations given every start-end combo exists as the inverse is valid, too except for Abandoned starts

So Freq.x is the count of trips from start to end
-- Freq.y is the count of trips from end to start
surplus is Freq.y - Freq.x which means that positive number indicates return trips are more and thus surplus of bikes
and negative number indicates return trips are less and thus the station is not breaking even

```{r}
res %>% filter(end_station_id == 'ABAN')
```

1363 rows indicate most start stations will have at least one abandoned bike

```{r}
res %>% filter(Freq.x == 0)
```

With Freq.x ==0 , there are 148,574  rows in which an end-to-start occurs, so the rows are pure surplus to the start_station_id

```{r}
# Want to identify the surplus (or not) by station for the month
station_surplus_count <- res %>%
  group_by(start_station_id) %>%
  summarize(surplus_sum=sum(surplus))
#station_surplus_count
```
The above indicates 1656 rows, 2 less than the count of end_station_id from the initial dataset

```{r}
station_surplus_count <- station_surplus_count[order(-station_surplus_count$surplus_sum),]
#station_surplus_count %>%
#    filter(startsWith(as.character(start_station_id), 'S'))
colnames(station_surplus_count)[1] <- "station_id"
#station_surplus_count
```
21 start with J
14 start with H

Above is result showing the surplus(+) or not surplus (-) of 1656 stations in the NYC district, this probably includes a few JC or HB stations

```{r}
sum(station_surplus_count$surplus_sum)
```

-8012 indicates 8012 abandons ... AND THAT IS A CORRECT NUMBER!!! phew!!!

```{r}
# Extract just the end station Id, lat, log
# because this had the higher count from the initial dataset, going with end_station_id
end_station_info <- citibike %>%
  select(end_station_id, end_lng, end_lat)

#end_station_info <- unique( end_station_info[ , 1:3 ] )
end_station_info <- end_station_info[!duplicated(end_station_info$end_station_id),]
end_station_info %>%
  filter(end_station_id == 'ABAN')
```

For awareness, the ABAN entry does have lng and lat
ABAN	-73.99	40.67	

```{r}
# Now I want the coordinates of all those station_ids
station_surplus_count_coords <- merge(x = station_surplus_count, y = end_station_info, by.x = 'station_id', by.y='end_station_id', all.x = TRUE)

colnames(station_surplus_count_coords)[3] <- "lng"
colnames(station_surplus_count_coords)[4] <- "lat"
#station_surplus_count_coords

station_surplus_count_coords <- station_surplus_count_coords %>%
  add_row(station_id = "ABAN", surplus_sum=1363, lng=-73.99, lat=40.67)

nrow(station_surplus_count_coords)
```
Above table accounts for all the station IDs with overal surplus or not surplus along with coordinates


```{r}
nrow(res)
```

310,625 * 2 - 619,887 = 1363 (number of abandon rows ... this is correct)

```{r}
# This removed the duplicate rows by transposing the start and end station ids
res_temp <- res %>% select(start_station_id, end_station_id)


res2 <- res_temp[!duplicated(lapply(as.data.frame(t(res_temp), stringsAsFactors=FALSE), sort)),]
head(res2, 20)
```
So now there are 310625 edges in the graph, a little over half ... which makes sense

so now I need to add back on the surplus by merging ... actually I don't need to add the values, these are the edges and that should be sufficient.


```{r}
g_citi <- graph_from_data_frame(res2, directed=FALSE, vertices=station_surplus_count_coords)
```

Nodes
id lon lat name

Edges
from to weight category

```{r}
edges_for_plot <- res2 %>%
  inner_join(station_surplus_count_coords %>% select(station_id, lng, lat), by=c('start_station_id' = 'station_id')) %>%
  rename(x=lng, y=lat) %>%
  inner_join(station_surplus_count_coords %>% select(station_id, lng, lat), by=c('end_station_id' = 'station_id')) %>%
  rename(xend=lng, yend=lat)

assert_that(nrow(edges_for_plot) == nrow(res2))
```

```{r}
#nodes$weight = degree(g)
```

```{r}
maptheme <- theme(panel.grid = element_blank()) +
  theme(axis.text = element_blank()) +
  theme(axis.ticks = element_blank()) +
  theme(axis.title = element_blank()) +
  theme(legend.position = "bottom") +
  theme(panel.grid = element_blank()) +
  theme(panel.background = element_rect(fill="#596673")) +
  theme(plot.margin = unit(c(0,0,0.5,0), 'cm'))
```

```{r}
country_shapes <- geom_polygon(aes(x=long, y=lat, group=group),
                               data=map_data('world'),
                               fill="#CECECE", color="#515151",
                               size=0.15)
mapcoords <- coord_fixed(xlim = c(-75, -73), ylim = c(40.5, 41))
```

```{r}
ggplot(station_surplus_count_coords) + country_shapes +
  geom_curve(aes(x = x, y = y, xend = xend, yend = yend),     # draw edges as arcs
             data = edges_for_plot, curvature = 0.33,
             alpha = 0.5) +
  scale_size_continuous(guide = FALSE, range = c(0.25, 2)) + # scale for edge widths
  geom_point(aes(x = lng, y = lat),           # draw nodes
             shape = 21, fill = 'white',
             color = 'black', stroke = 0.5) +
  scale_size_continuous(guide = "none", range = c(1, 6)) +    # scale for node size
  geom_text(aes(x = lng, y = lat, label = station_id),             # draw text labels
            hjust = 0, nudge_x = 1, nudge_y = 4,
            size = 3, color = "white", fontface = "bold") +
  mapcoords + maptheme
```
Well, there it is. Probably should be better, though.

# VALUE IN ASSESSING THE NETWORK GRAPH OF THE DATA, CLUSTER BY EDGES

-30- copy/paste from network graph work

# STATIONS

```{r}
# Ok, now I want to understand all the stations
n_distinct(citibike$start_station_id)
```

1618 start station IDs in NYC dataset

```{r}
n_distinct(citibike$end_station_id)
```

And 1658 end station IDs in NYC dataset. 40 more end_station_ids then start_station_ids.

```{r}
setdiff(citibike$end_station_id, citibike$start_station_id)
```

Of those 40, there is the ABAN, which means abandoned, several JC (Jersey City), several HB (Hoboken), one number which should be a specific station in NYC, and 3 SYS*

21 JC
1 ABAN
14 HB
1 NYC
3 SYS

### now to try leaflet to color the station Ids

```{r}
# Using basemaps for NYC
m <- leaflet(data=station_surplus_count_coords) %>% 
#  setView(zoom=12) %>%
  addTiles() %>%
  addCircleMarkers(
    ~lng, ~lat, 
    popup=~as.character(station.id), 
    label=~as.character(station.id), 
    radius=.5,
    color = ~ifelse(surplus.sum >= 1, 'blue', 
                    ifelse(surplus.sum == 0, 'green', 'red'))
  )


# Display map
m
```

Above map indicates the overall net bikes of a station for the given month
- blue, value is positive
- red, value is negative
- green, value is 0

```{r}
library(mapview)

## 'leaflet' objects (image above)
#m <- leaflet() %>% addTiles()
mapshot(m, file = "stations_surplus_sum.jpg")
```

```{r}
sum(station_surplus_count$surplus_sum)
```


From the initial dataset, there were 8012 trips without an station_id, so that means bikes were abandoned

```{r}
station_surplus_count %>%
  filter(surplus_sum > 0) %>%
  count()
```
799 stations with a surplus

```{r}
station_surplus_count %>%
  filter(surplus_sum == 0) %>%
  count()
```
53 stations that break even

```{r}
station_surplus_count %>%
  filter(surplus_sum < 0) %>%
  count()
```
804 stations with not surplus (overall result is less bikes)


804 + 53 + 799 = 1656

```{r}
head(station_surplus_count)
```

-30- End of tail of work from scrach_202209_v3.rmd which is really the leaflet work

# GOAL IS TO CREATE A MATRIX OF THE STARTS/ENDS FOR 15 MINUTE INTERVALS FOR EACH STATION


```{r}
nrow(citibike)
```


```{r}
citibike %>%
  filter(citibike$end_station_id  == 'ABAN')
```

```{r}
# Ok, need to convert start time to actual datetime format
#citibike$started_at <- as_datetime(as.character(citibike$started_at))
citibike$ended_at <- as_datetime(as.character(citibike$ended_at))
#citibike$end_station_name[citibike$end_station_name == ''] <- "Abandoned"
#citibike$end_station_id[citibike$end_station_id  == ''] <- "ABAN"

head(citibike)
```

```{r}
# And now for grouping by 15 minutes

citibike_by_start.interval <- citibike %>%
  mutate(start.interval=lubridate::floor_date(started_at, "15 min")) %>%
  group_by(start_station_id, start.interval) %>%
  summarize(start.count=n())

head(citibike_by_start.interval)
```

```{r}
citibike_by_end.interval <- citibike %>%
  mutate(end.interval=lubridate::floor_date(started_at, "15 min")) %>%
  group_by(end_station_id, end.interval) %>%
  summarize(count=n())

head(citibike_by_end.interval)
```

```{r}
# Now I Need to merge by station_id and time interval

citibike_by_interval <- 
  merge(citibike_by_start.interval, 
        citibike_by_end.interval, 
        by.x=c('start_station_id','start.interval'), 
        by.y=c('end_station_id','end.interval'), all = TRUE)
```



```{r}
citibike_by_interval[is.na(citibike_by_interval)] <- 0
citibike_by_interval
```

```{r}
# Rename columns
colnames(citibike_by_interval)[1] <- "station_id"
colnames(citibike_by_interval)[2] <- "interval_start.time"
colnames(citibike_by_interval)[3] <- "depart.count"
colnames(citibike_by_interval)[4] <- "arrive.count"

head(citibike_by_interval)
```

```{r}
# Calculate surplus
citibike_by_interval$surplus <- citibike_by_interval$arrive.count - citibike_by_interval$depart.count

head(citibike_by_interval)
```

```{r}
citibike_by_interval_surplus <- citibike_by_interval %>%
  select(station_id, interval_start.time, surplus)

head(citibike_by_interval_surplus)
```


```{r}
mydateseq<-seq(as.POSIXct("2022-09-01"), by="15 min", length.out = 2880)  

# Output the data sequence to confirm the result are 15 minute intervals which start on the hour
#mydateseq
```

```{r}
citibike_by_interval_surplus_wide <- citibike_by_interval_surplus %>%
  pivot_wider(names_from = station_id, values_from = surplus)

citibike_by_interval_surplus_wide[is.na(citibike_by_interval_surplus_wide)] <- 0


# For ordering
citibike_by_interval_surplus_wide <- citibike_by_interval_surplus_wide[order(citibike_by_interval_surplus_wide$interval_start.time),]
head(citibike_by_interval_surplus_wide)
```
# Nice the above matrix is by time and then for all the columns are stations with the plus/minus for each 15 minute interval

# Next is to count the running total

```{r}
citibike_by_interval_surplus_wide_cs <- cumsum(citibike_by_interval_surplus_wide[2:1659])
head(citibike_by_interval_surplus_wide_cs)
```

```{r}
time_index <- citibike_by_interval_surplus_wide$interval_start.time
#time_index
```


```{r}
# Write the above to CSV
#write.csv(citibike_by_interval_surplus_wide, file = 'stations_sept22_interval_surplus.csv')
```

```{r}

citibike_by_interval_surplus_wide_cs <- cbind(time_index, citibike_by_interval_surplus_wide_cs)
head(citibike_by_interval_surplus_wide_cs)
```

Alright, so `citibike_by_interval_surplus_wide_cs` is the running total of each station

```{r}
tail(citibike_by_interval_surplus_wide_cs)
```

# So many of the stations are negative, which can't be possible. Zero is absolutely the lowest number of bikes

-30- end of the scratch_matrix_15min_intervals_v1

# SHINY FOR LEAFLET

```{r}

```


```{r}
#Station_Information.json
# Read in Citi bike Station information and only use the 'stations' object of the JSON
citibike_station_raw <- jsonlite::fromJSON("Station_Information.json")
citibike_station_info <- citibike_station_raw$data$stations

# Only keep needed columns
citibike_station_info <- citibike_station_info %>%
  select("station_type", "short_name", "has_kiosk", "region_id", "name", "lat", "station_id", "external_id", "capacity", "lon", "electric_bike_surcharge_waiver", "legacy_id")

# Output to confirm data appears reasonable
head(citibike_station_info)
```

```{r}
citibike_station_info[is.na(citibike_station_info$lat),]
```

```{r}
#station_surplus_data <- read.csv('stations_sept22_interval_surplus.csv', row.names = 1, header= TRUE)
station_surplus_data <- citibike_by_interval_surplus_wide

# Output to confirm ... yes, it worked
head(station_surplus_data)
```

```{r eval=FALSE}
col_name <- as.data.frame(colnames(station_surplus_data))
colnames(col_name)[1] <- "short_name"

# Remove the 'X'
col_name <- gsub("X", "", col_name$short_name)

# Reset the column names
colnames(station_surplus_data) <- col_name

# Output to confirm ... yes, it worked
station_surplus_data
```

```{r}
# Pivot longer

station_surplus_data_long <- station_surplus_data %>%
  pivot_longer(!interval_start.time, names_to = "short_name", values_to = "surplus")

head(station_surplus_data_long)
```


```{r}
station_surplus_data_long$interval_start.time <- as.POSIXct(station_surplus_data_long$interval_start.time)
head(station_surplus_data_long)
```

2880 * 1658 = 4775040 ... so that worked


```{r}
# Select a date
selectedData <- station_surplus_data_long[station_surplus_data_long$interval_start.time == "2022-09-01 12:00:00",]

head(selectedData)
```
Ok, the above appears to have worked, I'm able to select on an interval and get just those rows

```{r}
citibike_nyc_station_info <- citibike_station_info
```

Ok, now I'm down to my 1658 stations ... phew


```{r}
citibike_nyc_station_info$Surplus <- selectedData$surplus[match(citibike_nyc_station_info$short_name, selectedData$short_name)]

head(citibike_nyc_station_info)
```


```{r}
# Create label texts
citibike_nyc_station_info$LabelText <- paste0(
  "<b>Station:</b> ", citibike_nyc_station_info$short_name, "<br>",
  "<b>Surplus:</b> ", format(citibike_nyc_station_info$Surplus, nsmall=0, big.mark = ",")
)
```

```{r eval=FALSE}
# For COVID DATA - PROBABLY DELETE!!
# Define color paletter for chart legend
paletteBins <- c(0, 50000, 100000, 250000, 500000, 1000000, 2500000, 5000000, 10000000)
colorPalette <- colorBin(palette = "YlOrBr", domain = covidData$Cumulative_cases, na.color = "transparent", bins = paletteBins)
```


```{r}
# Create map with circle markers
leaflet(citibike_nyc_station_info) %>%
  addTiles() %>%
#  setView(lat = 0, lng = 0, zoom=2) %>%
  
  addCircleMarkers(lng = ~lon,
                   lat = ~lat,
                   weight = 1,
                   opacity = 1,
                   radius=2,
                   color = ~ifelse(Surplus >= 1, 'blue', 
                      ifelse(Surplus == 0, 'green', 'red')),
                   label = ~lapply(LabelText, htmltools::HTML))
#%>%
  
#  leaflet::addLegend(pal=colorPalette, values=covidData$Cumulative_cases, opacity=0.9, title="Surplus by Station", position="bottomleft")
```

```{r}
missing_data <- citibike_nyc_station_info[is.na(citibike_nyc_station_info$lat),]
head(missing_data)
```

# Need to figure out the above!!! TODO

-30- end of scratch_leaflet_for_shiny_v1.rmd



```{r}
stations_with_elevation <- read.csv('stations_with_elevation.csv', row.names = 1, header= TRUE)
head(stations_with_elevation)
```

```{r}
stations_with_boro_hood <- read.csv('stations_with_boro_and_hood.csv', row.names = 1, header= TRUE)
head(stations_with_boro_hood)
```


```{r}
# Combine the elevation, borough, neighborhood, and September surplus
# First let's trim the DF for elevation
stations_with_elevation_trim <- stations_with_elevation %>%
  select(short_name, station_id, elevation, elev_units)

head(stations_with_elevation_trim)
```

```{r}
stations_with_boro_hood_trim <- stations_with_boro_hood %>%
  select(short_name, name, station_id, capacity, ntaname, boro_name, lon, lat)

head(stations_with_boro_hood_trim)
```


```{r}
stations_attrs_trim <- 
  merge(stations_with_elevation_trim, 
        stations_with_boro_hood_trim, 
        by.x=c('short_name'), 
        by.y=c('short_name'), all = TRUE)

stations_attrs_trim <- stations_attrs_trim %>%
  select(-station_id.y)

colnames(stations_attrs_trim)[colnames(stations_attrs_trim) == 'station_id.x'] <- 'station_id'

stations_attrs_trim[c("boro_name")][is.na(stations_attrs_trim[c("boro_name")])] <- "New Jersey"

stations_attrs_trim <- 
  stations_attrs_trim %>% 
  mutate(ntaname = ifelse(startsWith(short_name, "JC"), "Jersey City", ntaname))

stations_attrs_trim <- 
  stations_attrs_trim %>% 
  mutate(ntaname = ifelse(startsWith(short_name, "HB"), "Hoboken", ntaname))

head(stations_attrs_trim)
```
Above is good

```{r}
stations_attrs_trim %>%
  group_by(boro_name) %>%
  summarize(cnt=n())
```

```{r}
stations_attrs_trim %>%
  filter(boro_name == "New Jersey")
```

```{r}
#station_surplus_count
```

```{r}
stations_attrs_trim_sur <- 
  merge(stations_attrs_trim, 
        station_surplus_count, 
        by.x=c('short_name'), 
        by.y=c('station_id'), all.x = TRUE)

#stations_attrs_trim_sur
```

```{r}
stations_attrs_trim_sur <- stations_attrs_trim_sur %>%
  filter(!is.na(surplus_sum))

#stations_attrs_trim_sur
```

Hmm, I lost 74 by removing surplus_sum with NA, which actually removed a few NYC stations.

```{r}
# Station elevations
stations_attrs_trim_sur %>%
  filter(elevation < 100) %>%
  ggplot(aes(x=elevation)) +
  geom_histogram(bins=101) +
  labs(x = 'Elevation (meters)',
       y = 'Number of Stations')
```

```{r}
# Station surplus
stations_attrs_trim_sur %>%
  filter(surplus_sum >= -100 & surplus_sum <= 100) %>%
  ggplot(aes(x=surplus_sum)) +
  geom_histogram(bins=201) +
  labs(x = 'Surplus Count',
       y = 'Number of Stations')
```

```{r}
stations_attrs_trim_sur %>%
  filter(surplus_sum >= -100 & surplus_sum <= 100) %>%
  ggplot(aes(x=surplus_sum, color=boro_name)) +
  theme(legend.position="top") +
  geom_histogram(bins=201) +
  facet_wrap(~boro_name) +
  labs(x = 'Surplus Count',
       y = 'Number of Stations')

```

```{r}
# Network metrics
# https://rpubs.com/odenipinedo/network-analysis-in-R
#g_citi
```

```{r}
farthest_vertices(g_citi)
```

```{r}
get_diameter(g_citi) 
```

```{r}
# Calculate the out-degree of each vertex
g.outd <- degree(g_citi, mode = c("out"))
# Make a histogram of out-degrees
hist(g.outd, breaks = 30)
```


```{r}
# Find the vertex that has the maximum out-degree
which.max(g.outd)
```

```{r}
# Calculate betweenness of each vertex
g.b <- betweenness(g_citi, directed = TRUE)

# Show histogram of vertex betweenness
hist(g.b, breaks = 80)
```

```{r}
# Create plot with vertex size determined by betweenness score
plot(g_citi, 
     vertex.label = NA,
     edge.color = 'black',
     vertex.size = sqrt(g.b)+1,
     edge.arrow.size = 0.05,
     layout = layout_nicely(g_citi))

# Result is useless
```

```{r}

```

```{r}

```


```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```



```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```



```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```



```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```


~~~~~

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```
